# 19 图

|     19.1 理解图 |
| --- |
|     19.2 表示 |
|       19.2.1 按名称链接 |
|       19.2.2 按索引链接 |
|       19.2.3 边的列表 |
|       19.2.4 抽象表示 |
|     19.3 测量图的复杂性 |
|     19.4 可达性 |
|       19.4.1 简单递归 |
|       19.4.2 清理循环 |
|       19.4.3 带有内存的遍历 |
|       19.4.4 更好的接口 |
|     19.5 深度和广度优先遍历 |
|     19.6 具有加权边的图 |
|     19.7 最短路径（或最轻路径） |
|     19.8 摩拉维亚生成树 |
|       19.8.1 问题 |
|       19.8.2 一种贪心解决方案 |
|       19.8.3 另一种贪心解决方案 |
|       19.8.4 第三种解决方案 |
|       19.8.5 检查组件连通性 |

在从无环到循环中，我们介绍了一种特殊的共享方式：当数据变得循环时，即存在值，从这些值出发遍历其他可达值最终会回到开始的值。具有这种特性的数据称为图。在技术上，循环不一定是图；树或 DAG 也被视为（退化的）图。然而，在本节中，我们感兴趣的是具有循环潜力的图。

许多非常重要的数据是图。例如，社交媒体中的人物和关系形成一个图：人物是节点或顶点，而连接（例如友谊）是链接或边。它们形成一个图，因为对于许多人来说，如果你关注他们的朋友，然后关注朋友的朋友，最终会回到你开始的人。 （最简单地说，当两个人互相是朋友时，会发生这种情况。）同样，网络是一个图：节点是页面，边是页面之间的链接。互联网是一个图：节点是机器，边是机器之间的链接。交通网络是一个图：例如，城市是节点，边是它们之间的交通链接。等等。因此，理解图是表示和处理大量有趣的现实世界数据的必要条件。

图对于不仅实际而且也是原则性的原因都很重要和有趣。遍历可以回到起点的属性意味着传统的处理方法将不再适用：如果它盲目地处理每个访问的节点，它可能会陷入无限循环。因此，我们需要更好的结构化程序配方。此外，图具有非常丰富的结构，这使得它们可以进行几种有趣的计算。我们将在下文中研究图的这两个方面。

## 19.1 理解图

再次考虑我们之前见过的二叉树 [重新审视相等性]。现在让我们尝试扭曲“树”的定义，通过创建具有循环的树来创建，即，具有指向自身的节点的树（就相同性而言）。正如我们之前所见 [从无环到循环]，要创建这样的结构并不完全简单，但是我们之前见到的 [函数生成流] 可以帮助我们，在这里让我们推迟对循环链接的评估。也就是说，我们不仅要使用 rec，我们还必须使用一个函数来延迟评估。依此类推，我们必须更新字段上的注释。由于这些不再是“树”，我们将使用一个既具有暗示性又不完全不正确的名称：

```
data BinT:
  | leaf
  | node(v, l :: ( -> BinT), r :: ( -> BinT))
end
```

现在让我们试着构造一些循环值。以下是一些例子：

```
rec tr = node("rec", lam(): tr end, lam(): tr end)
t0 = node(0, lam(): leaf end, lam(): leaf end)
t1 = node(1, lam(): t0 end, lam(): t0 end)
t2 = node(2, lam(): t1 end, lam(): t1 end)
```

现在让我们试着计算 BinT 的大小。以下是显而易见的程序：

```
fun sizeinf(t :: BinT) -> Number:
  cases (BinT) t:
    | leaf => 0
    | node(v, l, r) =>
      ls = sizeinf(l())
      rs = sizeinf(r())
      1 + ls + rs
  end
end
```

(我们马上就会看到为什么我们把它称为 sizeinf。)

> 现在该做什么了！
> 
> > 当我们调用 sizeinf(tr) 时会发生什么？

它会进入一个无限循环：因此其名称中有 inf。

对于“大小”有两个非常不同的含义。一个是，“我们可以遍历边缘多少次？”另一个是，“作为数据结构的一部分构造了多少个不同的节点？”对于树来说，根据定义，这两个是相同的。对于有向无环图（DAG），前者超过了后者，但只是有限量的。对于一般图，前者可以无限地超过后者。对于像 tr 这样的数据，我们实际上可以无限次遍历边缘。但是构造的节点总数只有一个！让我们把这写成测试用例，以一个待定义的大小函数为基础：

```
check:
  size(tr) is 1
  size(t0) is 1
  size(t1) is 2
  size(t2) is 3
end
```

很明显，我们需要以某种方式记住我们以前访问过的节点：也就是说，我们需要一个带有“记忆”的计算。原则上这很容易：我们只需创建一个额外的数据结构来检查一个节点是否已经被计数。只要我们正确更新这个数据结构，我们就应该做好了准备。这里是一个实现。

```
fun sizect(t :: BinT) -> Number:
  fun szacc(shadow t :: BinT, seen :: List<BinT>) -> Number:
    if has-id(seen, t):
      0
    else:
      cases (BinT) t:
        | leaf => 0
        | node(v, l, r) =>
          ns = link(t, seen)
          ls = szacc(l(), ns)
          rs = szacc(r(), ns)
          1 + ls + rs
      end
    end
  end
  szacc(t, empty)
end
```

额外的参数 seen 被称为累加器，因为它“累积”了已见节点的列表。请注意，这也可以是一个集合；它不必是一个列表。它所需的支持函数检查给定节点是否已经被看到过：

```
fun has-id<A>(seen :: List<A>, t :: A):
  cases (List) seen:
    | empty => false
    | link(f, r) =>
      if f <=> t: true
      else: has-id(r, t)
      end
  end
end
```

这样做如何？嗯，sizect(tr)确实是 1，但 sizect(t1)是 3，sizect(t2)是 7！

> 现在开始！
> 
> > 解释为什么这些答案出现如此。

根本问题在于我们没有做好记忆！看看这一对行：

```
ls = szacc(l(), ns)
rs = szacc(r(), ns)

```

在遍历左分支时看到的节点实际上被遗忘了，因为在遍历右分支时我们记住的节点只有 ns 中的那些：即当前节点和那些访问过的“更高层”的节点。因此，任何“跨越边界”的节点都被计算了两次。

因此，解决这个问题的方法是记住我们访问的每个节点。然后，当我们没有更多节点要处理时，我们不仅返回大小，还应返回到目前为止访问的所有节点。这确保了具有多条路径的节点只在一条路径上被访问，而不是多次。这个逻辑是从每次遍历中返回两个值——<wbr>大小和所有访问的节点——<wbr>而不仅仅是一个。

```
fun size(t :: BinT) -> Number:
  fun szacc(shadow t :: BinT, seen :: List<BinT>)
    -> {n :: Number, s :: List<BinT>}:
    if has-id(seen, t):
      {n: 0, s: seen}
    else:
      cases (BinT) t:
        | leaf => {n: 0, s: seen}
        | node(v, l, r) =>
          ns = link(t, seen)
          ls = szacc(l(), ns)
          rs = szacc(r(), ls.s)
          {n: 1 + ls.n + rs.n, s: rs.s}
      end
    end
  end
  szacc(t, empty).n
end
```

毫无疑问，这个函数满足了上述测试。

## 19.2 表示

我们上面看到的图的表示方法当然是创建循环数据的一种开始，但并不十分优雅。到处写 lam 并记住将函数应用于()以获得实际值既容易出错又不优雅。因此，在这里我们探索其他更传统且更简单操作的图的表示方法。

有许多表示图的方法，选择表示方法取决于几个因素：

1.  图的结构，特别是其密度。我们稍后会进一步讨论这一点（衡量图的复杂性）。

1.  外部来源提供数据的表示方式。有时简单地适应它们的表示可能更容易；特别是在某些情况下甚至可能没有选择。

1.  编程语言提供的功能使得某些表示方法比其他方法更难使用。

之前[(部分“集合”)]，我们已经探讨了为一种数据类型有许多不同表示的想法。正如我们将看到的，这对图也是非常真实的。因此，最好是我们能够得到一个通用接口来处理图，这样所有后续程序都可以根据这个接口编写，而不过分依赖底层表示。在表示方面，我们需要三样主要的东西：

1.  一种构建图的方法。

1.  一种识别（即区分）图中节点或顶点的方法。

1.  给定一种识别节点的方法，获取该节点在图中的邻居的方法。

任何满足这些属性的接口都可以。为简单起见，我们将专注于这两个属性，而不是对构建图的过程进行抽象。

我们的运行示例将是一个图，其节点是美国的城市，边是它们之间已知的直接航班连接，类似于航空公司机上杂志背面的路线图。

### 19.2.1 按名称链接

这是我们的第一个表示。我们将假设每个节点都有一个唯一的名称（当这样的名称在数据存储库中用于查找信息时，有时被称为键）。然后，一个节点是一个键，关于该节点的一些信息，以及指向其他节点的键列表：

```
data KeyedNode:
  | keyed-node(key :: String, content, adj :: List<String>)
end

type KNGraph = List<KeyedNode>

type Node = KeyedNode
type Graph = KNGraph
```

（这里我们假设我们的键是字符串。）以下是这样一个图的具体实例：

```
kn-cities :: Graph = block:
  knEWR = keyed-node("nwk", "Newark", [list: "chi", "den", "saf", "hou"])
  knORD = keyed-node("chi", "Chicago", [list: "nwk", "saf"])
  knWOS = keyed-node("wor", "Worcester", [list: ])
  knHOU = keyed-node("hou", "Houston", [list: "nwk", "saf"])
  knDEN = keyed-node("den", "Denver", [list: "nwk", "saf"])
  knSFO = keyed-node("saf", "San Francisco", [list: "nwk", "den", "hou"])
  [list: knEWR, knORD, knWOS, knHOU, knDEN, knSFO]
end
```

给定一个键，这是我们查找其邻居的方法：

```
fun find-kn(key :: Key, graph :: Graph) -> Node:
  matches = for filter(n from graph):
    n.key == key
  end
  matches.first # there had better be exactly one!
end
```

> 练习
> 
> > 将函数中的注释转换为有关数据的不变式。将此不变式表达为一种细化，并将其添加到图的声明中。

有了这个支持，我们可以轻松地查找邻居：

```
fun kn-neighbors(city :: Key,  graph :: Graph) -> List<Key>:
  city-node = find-kn(city, graph)
  city-node.adj
end
```

在进行测试时，有些测试很容易编写。然而，其他一些测试可能需要描述整个节点，这可能很笨重，因此为了检查我们的实现，检查结果的一部分就足够了：

```
check:
  ns = kn-neighbors("hou", kn-cities)

  ns is [list: "nwk", "saf"]

  map(_.content, map(find-kn(_, kn-cities), ns)) is
    [list: "Newark", "San Francisco"]
end
```

### 19.2.2 按索引链接

在某些语言中，使用数字作为名称很常见。当数字可以用于在恒定时间内访问元素时（为了访问的元素数量有一个限制），这是特别有用的。在这里，我们使用一个列表—<wbr>它不提供对任意元素的恒定时间访问—<wbr>来说明这个概念。大部分内容看起来与之前的内容非常相似；我们将在最后评论一个关键的不同之处。

首先，数据类型：

```
data IndexedNode:
  | idxed-node(content, adj :: List<Number>)
end

type IXGraph = List<IndexedNode>

type Node = IndexedNode
type Graph = IXGraph
```

我们的图现在看起来像这样：

```
ix-cities :: Graph = block:
  inEWR = idxed-node("Newark", [list: 1, 4, 5, 3])
  inORD = idxed-node("Chicago", [list: 0, 5])
  inWOS = idxed-node("Worcester", [list: ])
  inHOU = idxed-node("Houston", [list: 0, 5])
  inDEN = idxed-node("Denver", [list: 0, 5])
  inSFO = idxed-node("San Francisco", [list: 0, 4, 3])
  [list: inEWR, inORD, inWOS, inHOU, inDEN, inSFO]
end
```

假设我们认为索引从 0 开始。要查找节点：

```
fun find-ix(idx :: Key, graph :: Graph) -> Node:
  index(graph, idx)
end
```

然后，我们几乎可以像以前一样找到邻居：

```
fun ix-neighbors(city :: Key,  graph :: Graph) -> List<Key>:
  city-node = find-ix(city, graph)
  city-node.adj
end
```

最后，我们的测试看起来也很相似：

```
check:
  ns = ix-neighbors(3, ix-cities)

  ns is [list: 0, 5]

  map(_.content, map(find-ix(_, ix-cities), ns)) is
    [list: "Newark", "San Francisco"]
end
```

这里发生了一些更深层次的事情。键控节点具有固有的键：键本身是数据的一部分。因此，仅仅给出一个节点，我们就可以确定其键。相比之下，索引节点表示外部键：键是在数据之外确定的，特别是由于在某些其他数据结构中的位置。给定一个节点而不是整个图，我们无法知道其键是什么。即使给出整个图，我们也只能通过使用 identical 来确定其键，这是一种对恢复基本信息的不太令人满意的方法。这凸显了使用外部键表示信息的弱点。（作为回报，外部键表示更容易重新组装成新的数据集合，因为不存在键冲突的危险：没有内在的键会冲突。）

### 19.2.3 边的列表

我们到目前为止看到的表示法优先考虑了节点，使边仅仅成为节点信息的一部分。相反，我们可以使用一种使边成为主要的表示法，而节点只是位于其末端的实体的表示法：

```
data Edge:
  | edge(src :: String, dst :: String)
end

type LEGraph = List<Edge>

type Graph = LEGraph
```

然后，我们的航班网络变为：

```
le-cities :: Graph =
  [list:
    edge("Newark", "Chicago"),
    edge("Newark", "Denver"),
    edge("Newark", "San Francisco"),
    edge("Newark", "Houston"),
    edge("Chicago", "Newark"),
    edge("Chicago", "San Francisco"),
    edge("Houston", "Newark"),
    edge("Houston", "San Francisco"),
    edge("Denver", "Newark"),
    edge("Denver", "San Francisco"),
    edge("San Francisco", "Newark"),
    edge("San Francisco", "Denver"),
    edge("San Francisco", "Houston") ]
```

要获取邻居的集合：

```
fun le-neighbors(city :: Key, graph :: Graph) -> List<Key>:
  neighboring-edges = for filter(e from graph):
    city == e.src
  end
  names = for map(e from neighboring-edges): e.dst end
  names
end
```

并确保： 

```
check:
  le-neighbors("Houston", le-cities) is
    [list: "Newark", "San Francisco"]
end
```

然而，这种表示方法使得存储节点的复杂信息变得困难，而不会复制它。因为节点通常具有丰富的信息，而边的信息倾向于较弱，我们通常更喜欢以节点为中心的表示。当然，另一种选择是将节点名称视为从中我们可以检索有关节点丰富信息的其他数据结构的键。

### 19.2.4 抽象表示

我们希望有一个通用的表示方法，让我们能够抽象出特定的实现。我们假设广泛地，我们有一个节点的概念，其中包含内容，一个键的概念（无论是内在的还是外在的），以及一种根据键和图给出邻居（<wbr>键列表—<wbr>）的方法。这对于接下来的工作是足够的。然而，我们仍然需要选择具体的键来编写示例和测试。为简单起见，我们将使用字符串键[按名称链接]。

## 19.3 度量图的复杂性

在我们开始定义图上的算法之前，我们应该考虑如何衡量图的大小。图有两个组成部分：节点和边。一些算法将专注于节点（例如，访问每个节点），而其他算法将专注于边，而有些算法将关注两者。那么我们用哪一个作为计数操作的基础：节点还是边？

如果我们能将这两个度量值简化为一个就好了。为了查看是否可能，假设一个图有\(k\)个节点。然后其边的数量有很大的范围：

+   没有两个节点相连。那么就根本没有边。

+   每两个节点都相连。然后边的数量基本上等于节点对的数量：大约\(k²\)。

一般来说，如果图的边远少于节点，则称为稀疏图，而如果边远多于节点，则称为稠密图。

因此，节点的数量可能显著少于或甚至显著多于边的数量。如果这种差异是常数问题，我们可以忽略它；但事实并非如此。对于稀疏图，节点的数量比边的数量多\(k\)倍（或者甚至是无穷大，如果确实没有边，但这样的图通常不太有趣或难以处理）；对于非常稠密的图，比值也是\(k\)的一个，但是方向相反。

因此，当我们想要讨论图上算法的复杂性时，我们必须考虑节点数量和边数量的大小。在一个连通图中，如果从每个节点出发，我们可以通过边到达每个其他节点，则边的数量至少与节点的数量相同，这意味着边的数量比节点的数量多。由于我们通常处理连通图，或者一次处理图的连通部分，我们可以将节点的数量限制在边的数量内。

## 19.4 可达性

许多图的用途需要解决可达性问题：即我们是否可以通过图中的边从一个节点到达另一个节点。例如，社交网络可能会建议所有可以从现有联系人到达的联系人。在互联网上，流量工程师关心数据包是否可以从一台机器到达另一台机器。在网络上，我们关心站点上的所有公共页面是否可以从主页到达。我们将以我们的旅行图作为一个运行示例来研究如何计算可达性。

### 19.4.1 简单递归

在其最简单的形式下，可达性很容易。我们想知道是否存在一条路径（路径是零个或多个链接边的序列）在一对节点之间，一个源节点和一个目标节点之间。 （更复杂的可达性版本可能计算实际路径，但我们暂时忽略这一点。）有两种可能性：源节点和目标节点相同，或者它们不同。

+   如果它们相同，那么显然可达性是显而易见的。

+   如果它们不同，我们必须遍历源节点的邻居，并询问目标节点是否可以从这些邻居中的每一个到达。

这转化为以下函数：<graph-reach-1-main> ::=

|   fun reach-1(src :: Key, dst :: Key, g :: Graph) -> Boolean: |
| --- |
|     if src == dst: |
|       true |
|     else: |
|       <graph-reach-1-loop> |
|       loop(neighbors(src, g)) |
|     end |
|   end |

循环通过 src 的邻居的过程是：<graph-reach-1-loop> ::=

|   fun loop(ns): |
| --- |
|     cases (List) ns: |
|       &#124; empty => false |
|       &#124; link(f, r) => |
|         if reach-1(f, dst, g): true else: loop(r) end |
|     end |
|   end |

我们可以按照以下方式进行测试：<graph-reach-tests> ::=

|   check: |
| --- |
|     reach = reach-1 |
|     reach("nwk", "nwk", kn-cities) 为 true |
|     reach("nwk", "chi", kn-cities) 为 true |
|     reach("nwk", "wor", kn-cities) 为 false |
|     reach("nwk", "hou", kn-cities) 为 true |
|     reach("nwk", "den", kn-cities) 为 true |
|     reach("nwk", "saf", kn-cities) 为 true |
|   end |

不幸的是，我们无法了解这些测试的结果，因为其中一些根本无法完成。这是因为我们有一个无限循环，由于图的循环性质！

> 练习
> 
> > 以上例子中哪一个导致了循环？为什么？

### 19.4.2 清理循环

在继续之前，让我们尝试改进循环的表达。虽然上面的嵌套函数是一个完全合理的定义，但我们可以使用 Pyret 的 for 来提高其可读性。

上述循环的本质是遍历一个布尔值列表；如果其中一个为 true，则整个循环评估为 true；如果它们全部为 false，则我们没有找到到达目标节点的路径，因此循环评估为 false。因此：

```
fun ormap(fun-body, l):
  cases (List) l:
    | empty => false
    | link(f, r) =>
      if fun-body(f): true else: ormap(fun-body, r) end
  end
end
```

有了这个，我们可以替换循环定义和使用为：

```
for ormap(n from neighbors(src, g)):
  reach-1(n, dst, g)
end
```

### 19.4.3 带有记忆的遍历

由于我们有循环数据，我们必须记住我们已经访问过的节点，并避免再次遍历它们。因此，每次我们开始遍历一个新节点时，我们将其添加到我们已经开始访问的节点集合中。如果我们返回到该节点，因为我们可以假设图在此期间没有发生变化，我们知道从该节点进行的额外遍历不会对结果产生任何影响。这种属性被称为☛幂等性。

因此，我们定义了第二次尝试可达性，它需要一个额外的参数：我们已经开始访问的节点集合（其中集合表示为图）。与<graph-reach-1-main>的关键区别在于，在我们开始遍历边之前，我们应该检查是否已经开始处理该节点。这导致以下定义：<graph-reach-2> ::=

|   fun reach-2(src :: Key, dst :: Key, g :: Graph, visited :: List<Key>) -> Boolean: |
| --- |
|     if visited.member(src): |
|       false |
|     else if src == dst: |
|       true |
|     else: |
|       new-visited = link(src, visited) |
|       for ormap(n from neighbors(src, g)): |
|         reach-2(n, dst, g, new-visited) |
|       end |
|     end |
|   end |

特别要注意额外的新条件：如果可达性检查已经访问过此节点，则从这里进一步遍历没有意义，因此返回 false。（可能仍然有其他部分的图需要探索，其他递归调用会处理。）

> 练习
> 
> > 如果前两个条件交换了会有什么影响，即，reach-2 的开始是否以以下方式开始
> > 
> > ```
> > if src == dst:
> >   true
> > else if visited.member(src):
> >   false
> > ```
> > 
> > ? 用具体示例解释。
> > 
> 练习
> 
> > 我们反复谈论我们已经开始访问的节点，而不是我们已经完成访问的节点。这种区别重要吗？为什么？

### 19.4.4 更好的接口

正如测试 reach-2 的过程所示，我们可能有一个更好的实现，但我们改变了函数的接口；现在它有一个不必要的额外参数，这不仅是一个麻烦，而且如果我们意外地误用它，可能会导致错误。因此，我们应该通过将核心代码移动到内部函数来清理我们的定义：

```
fun reach-3(s :: Key, d :: Key, g :: Graph) -> Boolean:
  fun reacher(src :: Key, dst :: Key, visited :: List<Key>) -> Boolean:
    if visited.member(src):
      false
    else if src == dst:
      true
    else:
      new-visited = link(src, visited)
      for ormap(n from neighbors(src, g)):
        reacher(n, dst, new-visited)
      end
    end
  end
  reacher(s, d, empty)
end
```

现在我们已经恢复了原始接口，同时正确实现了可达性。

> 练习
> 
> > 这样真的给我们一个正确的实现吗？特别是，这是否解决了上面的 size 函数所解决的问题？创建一个展示问题的测试用例，然后修复它。

## 19.5 深度优先和广度优先遍历

计算机科学文本通常将这些称为深度优先和广度优先搜索。然而，搜索只是一个特定目的；遍历是一个通用任务，可以用于许多目的。

我们上面看到的可达性算法有一个特殊的属性。在它访问的每个节点，通常存在一组相邻节点，它可以继续遍历。它至少有两个选择：它可以首先访问每个直接邻居，然后访问所有邻居的邻居；或者它可以选择一个邻居，进行递归，并且仅在该访问完成后才访问下一个直接邻居。前者被称为广度优先遍历，而后者被称为深度优先遍历。

我们设计的算法使用深度优先策略：在 <graph-reach-1-loop> 中，我们在访问第二个邻居之前对邻居列表的第一个元素进行递归，依此类推。另一种方法是有一个数据结构，我们将所有邻居插入其中，然后每次取出一个元素，这样我们首先访问所有邻居的邻居，依此类推。这自然对应于一个队列（一个例子：从列表创建队列）。

> 练习
> 
> > 使用队列，实现广度优先遍历。

如果我们正确检查以确保不重新访问节点，那么广度优先和深度优先遍历都将正确访问整个可达图而不重复（因此不会陷入无限循环）。每个遍历从一个节点只遍历一次，从中考虑每一条边。因此，如果一个图有\(N\)个节点和\(E\)条边，那么遍历的复杂度下界为\(O([N, E \rightarrow N + E])\)。我们还必须考虑检查我们是否已经访问过一个节点的成本（这是一个集合成员问题，在其他地方我们会解决：（部分"sets"））。最后，我们必须考虑维护跟踪我们遍历的数据结构的成本。在深度优先遍历的情况下，递归——<wbr>使用机器的栈——<wbr>会以恒定的开销自动完成。在广度优先遍历的情况下，程序必须管理队列，这可能会增加超过常数的开销。实际上，栈通常比队列表现得更好，因为它得到了机器硬件的支持。

这可能暗示深度优先遍历总是比广度优先遍历更好。然而，广度优先遍历具有一个非常重要且有价值的特性。从节点\(N\)开始，当它访问节点\(P\)时，计算到达\(P\)的边的数量。广度优先遍历保证不可能有一条更短的路径到达\(P\)：也就是说，它找到了一条最短路径到\(P\)。

> 练习
> 
> > 为什么是“a”而不是“the”最短路径？
> > 
> 现在动手！
> 
> > 证明广度优先遍历找到最短路径。

## 19.6 带有加权边的图

考虑一个交通图：我们通常不仅关心是否可以从一个地方到达另一个地方，还关心“成本”（我们可能有许多不同的成本度量：金钱、距离、时间、二氧化碳单位等）。在互联网上，我们可能关心链接的☛延迟或☛带宽。甚至在社交网络中，我们可能想描述朋友之间的亲密程度。简而言之，在许多图中，我们不仅对边的方向感兴趣，还对一些抽象的数值度量感兴趣，我们称之为权重。

在本研究的其余部分中，我们将假设我们的图边具有权重。这并不否定我们迄今所学的内容：如果在无权图中可达一个节点，在带权图中仍然可达。但是我们将在下面要研究的操作只在带权图中才有意义。然而，我们总是可以通过给每条边赋予相同的常量正权重（比如说一）来将无权图视为带权图。

> 练习
> 
> > 当将无权图视为带权图时，为什么我们关心每条边都被赋予正权重？
> > 
> 练习
> 
> > 修改图的数据定义以考虑边的权重。
> > 
> 练习
> 
> > 权重并不是我们可能记录关于边的唯一类型的数据。例如，如果图中的节点代表人，边可以用他们的关系标记（“母亲”，“朋友”等）。你能想象记录边的其他什么类型的数据吗？

## 19.7 最短（或最轻）路径

想象一下规划一次旅行：你可能希望以最短的时间、最少的金钱或其他一些涉及最小化边权重总和的标准到达目的地。这被称为计算最短路径。

我们应该立即澄清一个不幸的术语混淆。我们真正想计算的是最轻的路径——*最小权重*的路径。不幸的是，计算机科学术语已经确定了我们在这里使用的术语；只要确保不要字面理解即可。

> 练习
> 
> > 构建一个图并选择其中的一对节点，使得从一个节点到另一个节点的最短路径不是最轻的，反之亦然。

我们已经看到（深度和广度优先遍历）广度优先搜索在无权图中构建最短路径。当没有权重时，这对应于最轻的路径（或者等价地，所有权重都相同且为正）。现在我们必须将此推广到边具有权重的情况。

我们将逐步归纳地定义一个似乎是这种类型的函数。

```
w :: Key -> Number
```

这反映了从源节点到该节点的最轻路径的权重。但是让我们思考一下这个注释：因为我们是逐个节点构建这个注释的，最初大多数节点都没有权重可报告；甚至到最后，从源节点无法到达的节点也没有最轻路径的权重（或者任何路径）。我们不会编造一个假装反映这种情况的数字，而是使用一个选项类型：

```
w :: Key -> Option<Number>
```

当有一些值时，它将是权重；否则权重将为 none。

现在让我们归纳地思考一下。最初我们知道什么？嗯，当然是源节点与自身的距离为零（这必须是最轻的路径，因为我们无法再轻了）。这给了我们一组（微不足道的）节点，我们已经知道了最轻的权重。我们的目标是扩展这组节点——<wbr>逐次增加一个节点，直到我们找到目标，或者我们没有更多的节点可添加（在这种情况下，我们的目的地从源节点不可达）。

归纳地，每一步我们都有一组我们知道最轻路径的所有节点（最初只是源节点，但这意味着这个集合永远不会为空，在接下来我们要说的话中这一点很重要）。现在考虑所有邻接到这组节点的边，这些边通向我们还不知道最轻路径的节点。选择一个节点\(q\)，使到它的路径的总权重最小。我们声明，这实际上将是通向该节点的最轻路径。

如果这一说法成立，那么我们就完成了。因为我们现在将\(q\)添加到我们现在已知最轻权重的节点集中，并重复从那里找到最轻的出边的过程。这个过程因此添加了一个更多的节点。在某个时候，我们将发现没有边通向已知集之外，此时我们可以终止。

毫无疑问，在这一点上终止是安全的：这对应于计算到达集的情况。唯一剩下的就是证明这个贪婪算法给每个节点都找到了最轻的路径。

我们将通过反证法证明这一点。假设我们有路径\(s \rightarrow d\)从源\(s\)到节点\(d\)，如上述算法所找到的，但也假设我们有一条实际上更轻的不同路径。在每个节点上，当我们沿着\(s \rightarrow d\)路径添加一个节点时，如果存在更轻的路径，算法将会添加更轻的路径。它没有找到更轻的路径，这一事实证明了我们存在更轻路径的说法是错误的（可能存在相同权重的不同路径；这在算法中是允许的，但这也不会与我们的说法相矛盾）。因此该算法确实找到了最轻的路径。

现在需要确定一种数据结构，使得该算法能够实现。在每个节点，我们想要知道从已知最小权重到所有邻居节点的节点集的最小权重。我们可以通过排序来实现这一点，但这有点杀鸡用牛刀：实际上，我们并不需要对所有这些权重进行完全排序，只需要知道最轻的那个。一个堆[REF]能够满足我们的需求。

> 练习
> 
> > 如果我们允许边的权重为零会怎样？上述算法会发生什么变化？
> > 
> 练习
> 
> > 如果我们允许负权重的边会怎样？上述算法会发生什么变化？

供您参考，此算法被称为 Dijkstra 的算法。

## 19.8 摩拉维亚覆盖树

在千禧年之交，美国国家工程院调查其成员，确定“20 世纪最伟大的工程成就”。列表中包含了一些常见的：电子、计算机、互联网等等。但一个也许令人惊讶的想法排在了榜首：（农村）电气化。详细了解请查看[他们的网站](http://www.greatachievements.org/)。

### 19.8.1 问题

要理解国家电网的历史，最好回到 20 世纪 20 年代的[摩拉维亚](http://en.wikipedia.org/wiki/Moravia)。和世界上许多地方一样，这里开始意识到电力的好处，并打算将其传播到该地区。一位摩拉维亚的学者名叫奥塔卡尔·博鲁夫卡听说了这个问题，并在一次非凡的努力中，将问题抽象地描述出来，以便可以不参考摩拉维亚或电网的情况下理解。他将其建模为一个关于图的问题。

Borůvka 观察到，至少在最初，创建网络的任何解决方案必须具有以下特征：

+   电网必须覆盖到所有预期覆盖的城镇。用图的术语来说，解决方案必须是覆盖的，也就是说，它必须访问图中的每个节点。

+   冗余是任何网络中的一种有价值的属性：这样，如果一组链接失效，可能会有另一种方式将有效负载传送到目的地。然而，在起步阶段，冗余可能太昂贵了，特别是如果以完全不给某人有效负载为代价的话。因此，最初的解决方案最好没有环路，甚至没有冗余路径。用图的术语来说，解决方案必须是一棵树。

+   最后，目标是以尽可能低的成本解决这个问题。用图的术语来说，图将被加权，解决方案必须是最小的。

因此，Borůvka 定义了摩拉维亚覆盖树（MST）问题。

### 19.8.2 贪婪算法

博鲁夫卡已经发表了他的问题，另一位捷克数学家[沃伊切赫·亚尔尼克](http://en.wikipedia.org/wiki/Vojt%C4%9Bch_Jarn%C3%ADk)偶然间发现了它。亚尔尼克提出的解决方案应该听起来很熟悉：

+   从一个任意选择的单个节点开始，构建一个解决方案。对于仅包含这一个节点的图来说，这个解决方案显然是最小的、覆盖的，也是一棵树。

+   在连接到尚未包含在解决方案中的节点的解决方案中所有边中选择具有最小权重的边。请注意，我们仅考虑相邻的边，而不考虑它们的权重加到它们相邻的节点的权重上。

+   将这条边添加到解决方案中。声明是，对于新解决方案，将是一棵树（通过构造），跨度（也通过构造），并且是最小的。最小性是通过类似于迪杰斯特拉算法使用的论据得到的。

雅尔尼克不幸的是在 1930 年用捷克语发表了这项工作，并且它基本上被忽视了。它被其他人重新发现，特别是在 1957 年被 R.C. 普林重新发现，现在通常称为普林姆算法，尽管称其为雅尔尼克算法将把功劳归于正确的地方。

实现这个算法相当容易。在每个点，我们需要知道与当前解决方案树相邻的最轻的边。找到最轻的边需要与这些边的数量成正比的时间，但是最轻的边可能会创建一个循环。因此，我们需要有效地检查添加边是否会创建循环，这是一个我们将多次返回的问题（检查组件连通性）。假设我们可以有效地解决这个问题，然后我们想要添加最轻的边并进行迭代。即使考虑到一个有效的检查循环性的解决方案，这似乎也需要对每个节点进行与边数成正比的操作。通过更好的表示，我们可以改进这种复杂性，但是让我们首先看看其他想法。

### 另一个贪婪解决方案

请记住，雅尔尼克是在 1930 年提出他的算法的，那时计算机还不存在，普林姆是在 1957 年提出他的算法的，那时计算机正处于发展初期。编程跟踪堆是一个非常棘手的问题，许多算法是手工实现的，其中对复杂数据结构进行跟踪而不出错更加困难。需要一种需要较少手工簿记（从字面上讲）的解决方案。

在 1956 年，[Joseph Kruskal](http://en.wikipedia.org/wiki/Joseph_Kruskal) 提出了这样一个解决方案。他的想法非常简洁。雅尔尼克算法遭受的问题是每次树增长时，我们都必须修订堆的内容，这已经是一个混乱的结构要跟踪。克鲁斯卡尔指出了以下事实。

要获得最小解，我们肯定希望在图中包含一条最小权重的边。因为如果不包括，我们可以取一个否则是最小解，加上这条边，再移除另一条边；图仍然保持连接，但总体权重不会增加，而且如果移除的边更重，那么总体权重会减少。注意仔细的措辞：可能有许多权重相同的边，因此添加其中一条可能会移除另一条，因此不能产生更轻的树；但关键是它肯定不会产生更重的树。通过相同的论证，我们可以添加下一个最轻的边，以此类推。唯一不能添加下一个最轻的边的情况是当它会创建一个循环（再次出现这个问题！）。

因此，克鲁斯卡尔算法是非常直观的。我们首先对所有边按升序权重排序。然后，按照升序权重顺序取每条边，并将其添加到解决方案中，只要它不会创建循环。当我们处理完所有边时，我们将得到一个树形解决方案（通过构造），跨度（因为每个连接的顶点必须是某条边的端点），并且具有最小权重（根据上述论证）。复杂度是排序的复杂度（这是\([e \rightarrow e \log e]\)，其中\(e\)是边集的大小）。然后我们对\(e\)中的每个元素进行迭代，这需要与该集合大小成线性关系的时间，除了检查循环所需的时间。这个算法也很容易在纸上实现，因为我们只需要一次对所有边排序，然后按顺序检查它们，划掉创建循环的边—<wbr>不需要对列表进行动态更新。

### 19.8.4 第三种解决方案

雅尼克和克鲁斯卡尔的解决方案都有一个缺陷：它们需要一个集中的数据结构（优先堆或排序列表）来逐步构建解决方案。随着并行计算机的出现，以及图问题的规模增大，计算机科学家开始寻找可以更高效地在并行环境中实现的解决方案—<wbr>这通常意味着避免任何集中的同步点，例如这些集中的数据结构。

1965 年，M. Sollin 构建了一个完美满足这些需求的算法。在这个算法中，我们不是构建单个解决方案，而是增长多个解决方案组件（如果愿意，可能是并行的）。每个节点最初都作为一个解决方案组件（就像雅尼克算法的第一步）。每个节点考虑与其相连的边，并选择连接到不同组件的最轻的边（再次出现这个问题！）。如果可以找到这样的边，则该边成为解决方案的一部分，并且两个组件合并为一个单一的组件。整个过程重复进行。

因为每个节点都作为解的一部分开始，这个算法自然地跨越。因为它检查循环并避免它们，它自然地形成一棵树。注意，避免循环会产生一个有向无环图，并不会自动保证产生一棵树。在本节中，我们对这个区别有些放松。最后，最小性通过类似的推理得到，就像我们在贾尔尼克算法的情况下使用的那样，我们基本上是并行运行的，每次从每个节点开始，直到并行解决方案组件汇聚起来产生一个全局解决方案。

当然，手动维护这个算法的数据是一场噩梦。因此，这个算法是在数字时代创造出来的不足为奇。因此，真正的惊喜是，它原来不是这样的：它最初是由[Otakar Borůvka](http://en.wikipedia.org/wiki/Otakar_Bor%C5%AFvka)本人创建的。

你看，博鲁夫卡已经搞清楚了一切。他不仅理解了问题，他还：

+   确定了电气化问题下面的真正问题，以便可以从与上下文无关的方式来看待它，

+   创建了一个描述性的图论语言来精确定义它，

+   甚至解决了问题，而不仅仅是定义它。

他只是想出了一个解决方案，以至于 Jarník 本质上已经将其解除了并行化，以便可以按顺序执行。因此，直到被 Sollin 重新发现（[实际上，多次重新发现](http://en.wikipedia.org/wiki/Bor%C5%AFvka's_algorithm)）以适应并行计算人员的需求，这个算法才被注意到。但现在我们只需要称呼这个为博鲁夫卡算法，这是理所当然的。

你现在可能已经猜到了，这个问题在其他教科书中确实被称为最小生成树（MST），但“M”不是指摩拉维亚，而是指“最小”。但考虑到博鲁夫卡在历史上的被忽视地位，我更喜欢更古怪的名字。

### 检查组件连接性

正如我们所见，我们需要能够有效地确定两个节点是否在同一个组件中。一种方法是从第一个节点开始进行深度优先遍历（或广度优先遍历），然后检查我们是否曾经访问过第二个节点。（使用这些遍历策略之一可以确保在存在循环时终止。）不幸的是，这对每对节点需要花费线性的时间（与图的大小成比例）-并且根据图的情况和节点的选择，我们可能会对图中的每个节点在每次边添加时都这样做！所以我们显然希望做得更好。

将这个问题从图连接性简化为更一般的问题很有帮助：不相交集结构（俗称并查集，原因很快就会清楚）。如果我们把每个连通分量看作一个集合，那么我们正在询问两个节点是否在同一个集合中。但将其转化为一个集合成员问题使得它在其他几个应用中也适用。

设置如下。对于任意值，我们希望能够将它们视为集合中的元素。我们感兴趣的是两个操作。一个显然是并集，它将两个集合合并为一个。另一个看起来像是 is-in-same-set，它接受两个元素并确定它们是否在同一个集合中。然而，随着时间的推移，实际上定义了一个更有用的操作，即 find 运算符，它给定一个元素，“命名”了元素所属的集合（稍后再说）。要检查两个元素是否在同一个集合中，我们需要获取每个元素的“集合名称”，并检查这些名称是否相同。这听起来固然有些绕弯，但这意味着我们有了一个在其他情况下可能有用的原始操作，并且我们可以很容易地从中实现 is-in-same-set。

现在的问题是，我们如何命名集合？我们真正应该问的问题是，我们关心对这些名称执行什么操作？我们关心的只是，给定两个名称，当且仅当这两个名称相同时，它们确实代表相同的集合。因此，我们可以构造一个新的字符串、或者数字，或者其他东西，但我们还有另一个选择：简单地选择集合的某个元素来代表它，即，作为它的名称。因此，我们将每个集合元素与该元素的“集合名称”指示符关联起来；如果没有，则它的名称就是它自己（父对象的 none 情况）：

```
data Element<T>:
  | elt(val :: T, parent :: Option<Element>)
end
```

我们将假设我们有一些相等谓词，用于检查两个元素何时相同，我们通过比较它们的值部分来执行此操作，忽略它们的父值：

```
fun is-same-element(e1, e2): e1.val <=> e2.val end
```

> 现在动手做！
> 
> > 为什么我们只检查值部分？

我们将假设对于给定的集合，我们始终返回相同的代表元素。（否则，即使我们有相同的集合，相等性也会失败。）因此：我们使用了名称 fynd，因为在 Pyret 中，find 已经被定义为表示其他意义。

```
fun is-in-same-set(e1 :: Element, e2 :: Element, s :: Sets)
    -> Boolean:
  s1 = fynd(e1, s)
  s2 = fynd(e2, s)
  identical(s1, s2)
end
```

其中 Sets 是所有元素的列表：

```
type Sets = List<Element>
```

我们如何找到集合的代表元素？我们首先使用 is-same-element 找到它；当我们这样做时，我们检查元素的父字段。如果它是 none，那么这意味着这个元素就是它的集合的名称；这可能是因为元素是一个单例集合（我们将所有元素初始化为 none），或者它是一些较大集合的名称。无论哪种情况，我们都完成了。否则，我们必须递归地找到父对象：

```
fun fynd(e :: Element, s :: Sets) -> Element:
  cases (List) s:
    | empty => raise("fynd: shouldn't have gotten here")
    | link(f, r) =>
      if is-same-element(f, e):
        cases (Option) f.parent:
          | none => f
          | some(p) => fynd(p, s)
        end
      else:
        fynd(e, r)
      end
  end
end
```

> 练习
> 
> > 为什么这在嵌套情况下是递归的？

实现并查集的剩余部分就是要实现并。为此，我们找到两个要并的集合的代表元素；如果它们相同，那么这两个集合已经在一个并集中；否则，我们必须更新数据结构：

```
fun union(e1 :: Element, e2 :: Element, s :: Sets) -> Sets:
  s1 = fynd(e1, s)
  s2 = fynd(e2, s)
  if identical(s1, s2):
    s
  else:
    update-set-with(s, s1, s2)
  end
end
```

要更新，我们任意选择一个集合名称作为新复合集合的名称。然后，我们必须将另一个集合的名称元素的父对象更新为此集合的名称：

```
fun update-set-with(s :: Sets, child :: Element, parent :: Element)
    -> Sets:
  cases (List) s:
    | empty => raise("update: shouldn't have gotten here")
    | link(f, r) =>
      if is-same-element(f, child):
        link(elt(f.val, some(parent)), r)
      else:
        link(f, update-set-with(r, child, parent))
      end
  end
end
```

这里有一些测试来说明这个工作原理：

```
check:
  s0 = map(elt(_, none), [list: 0, 1, 2, 3, 4, 5, 6, 7])
  s1 = union(index(s0, 0), index(s0, 2), s0)
  s2 = union(index(s1, 0), index(s1, 3), s1)
  s3 = union(index(s2, 3), index(s2, 5), s2)
  print(s3)
  is-same-element(fynd(index(s0, 0), s3), fynd(index(s0, 5), s3)) is true
  is-same-element(fynd(index(s0, 2), s3), fynd(index(s0, 5), s3)) is true
  is-same-element(fynd(index(s0, 3), s3), fynd(index(s0, 5), s3)) is true
  is-same-element(fynd(index(s0, 5), s3), fynd(index(s0, 5), s3)) is true
  is-same-element(fynd(index(s0, 7), s3), fynd(index(s0, 7), s3)) is true
end
```

不幸的是，这种实现遇到了两个主要问题：

+   首先，因为我们正在执行函数式更新，父引用的值保持“变化”，但这些变化对于“相同”值的旧副本是不可见的。来自不同联合阶段的元素具有不同的父引用，尽管在整个过程中它们可能是相同的元素。这是函数式编程带来问题的地方。

+   相关的是，这个实现的性能非常糟糕。fynd 递归地遍历父级以找到集合的名称，但遍历的元素没有更新以记录这个新名称。我们当然可以每次重新构建集合来更新它们，但这会使实现变得复杂，并且，正如我们很快会看到的，我们可以做得更好。

底线是，纯函数式编程与这个问题并不是很匹配。我们需要一个更好的实现策略：不相交集合重访。

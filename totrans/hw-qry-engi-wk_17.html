<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Query Execution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Query Execution</h1>
<blockquote>原文：<a href="https://howqueryengineswork.com/13-execution.html">https://howqueryengineswork.com/13-execution.html</a></blockquote>
                        
<p><em>The source code discussed in this chapter can be found in the <code>execution</code> module of the <a href="https://github.com/andygrove/how-query-engines-work">KQuery project</a>.</em></p>
<p>We have built all the pieces: data sources, logical plans, physical plans, a query planner, and an optimizer. This chapter ties them together into a working query engine.</p>
<h2 id="the-execution-context"><a class="header" href="#the-execution-context">The Execution Context</a></h2>
<p>The <code>ExecutionContext</code> is the entry point for running queries. It manages registered tables and coordinates the execution pipeline:</p>
<pre><code class="language-kotlin">class ExecutionContext(val settings: Map&lt;String, String&gt;) {

    private val tables = mutableMapOf&lt;String, DataFrame&gt;()

    fun sql(sql: String): DataFrame {
        val tokens = SqlTokenizer(sql).tokenize()
        val ast = SqlParser(tokens).parse() as SqlSelect
        return SqlPlanner().createDataFrame(ast, tables)
    }

    fun csv(filename: String): DataFrame {
        return DataFrameImpl(Scan(filename, CsvDataSource(filename, ...), listOf()))
    }

    fun register(tablename: String, df: DataFrame) {
        tables[tablename] = df
    }

    fun execute(plan: LogicalPlan): Sequence&lt;RecordBatch&gt; {
        val optimizedPlan = Optimizer().optimize(plan)
        val physicalPlan = QueryPlanner().createPhysicalPlan(optimizedPlan)
        return physicalPlan.execute()
    }
}
</code></pre>
<p>Users interact with the context to:</p>
<ol>
<li>Register data sources as named tables</li>
<li>Build queries using SQL or the DataFrame API</li>
<li>Execute queries and consume results</li>
</ol>
<h2 id="the-execution-pipeline"><a class="header" href="#the-execution-pipeline">The Execution Pipeline</a></h2>
<p>When you execute a query, it flows through several stages:</p>
<pre><code>SQL String
    ↓ tokenize
Tokens
    ↓ parse
SQL AST
    ↓ plan
Logical Plan
    ↓ optimize
Optimized Logical Plan
    ↓ physical planning
Physical Plan
    ↓ execute
Sequence&lt;RecordBatch&gt;
</code></pre>
<p>For DataFrame queries, the pipeline starts at the logical plan stage since the DataFrame API builds logical plans directly.</p>
<h3 id="stage-1-parsing-sql-only"><a class="header" href="#stage-1-parsing-sql-only">Stage 1: Parsing (SQL only)</a></h3>
<p>SQL text becomes tokens, then a syntax tree:</p>
<pre><code class="language-kotlin">val tokens = SqlTokenizer(sql).tokenize()
val ast = SqlParser(tokens).parse()
</code></pre>
<h3 id="stage-2-logical-planning"><a class="header" href="#stage-2-logical-planning">Stage 2: Logical Planning</a></h3>
<p>The SQL AST (or DataFrame) becomes a logical plan:</p>
<pre><code class="language-kotlin">val logicalPlan = SqlPlanner().createDataFrame(ast, tables).logicalPlan()
</code></pre>
<h3 id="stage-3-optimization"><a class="header" href="#stage-3-optimization">Stage 3: Optimization</a></h3>
<p>The optimizer transforms the logical plan:</p>
<pre><code class="language-kotlin">val optimizedPlan = Optimizer().optimize(logicalPlan)
</code></pre>
<h3 id="stage-4-physical-planning"><a class="header" href="#stage-4-physical-planning">Stage 4: Physical Planning</a></h3>
<p>The query planner creates an executable physical plan:</p>
<pre><code class="language-kotlin">val physicalPlan = QueryPlanner().createPhysicalPlan(optimizedPlan)
</code></pre>
<h3 id="stage-5-execution"><a class="header" href="#stage-5-execution">Stage 5: Execution</a></h3>
<p>The physical plan executes, producing record batches:</p>
<pre><code class="language-kotlin">val results: Sequence&lt;RecordBatch&gt; = physicalPlan.execute()
</code></pre>
<h2 id="running-a-query"><a class="header" href="#running-a-query">Running a Query</a></h2>
<p>Here is a complete example:</p>
<pre><code class="language-kotlin">val ctx = ExecutionContext(mapOf())

// Register a CSV file as a table
ctx.registerCsv("employees", "/data/employees.csv")

// Execute a SQL query
val df = ctx.sql("""
    SELECT department, AVG(salary) as avg_salary
    FROM employees
    WHERE state = 'CO'
    GROUP BY department
""")

// Execute and print results
ctx.execute(df).forEach { batch -&gt;
    println(batch.toCSV())
}
</code></pre>
<p>Or using the DataFrame API:</p>
<pre><code class="language-kotlin">val ctx = ExecutionContext(mapOf())

val results = ctx.csv("/data/employees.csv")
    .filter(col("state") eq lit("CO"))
    .aggregate(
        listOf(col("department")),
        listOf(avg(col("salary")))
    )

ctx.execute(results).forEach { batch -&gt;
    println(batch.toCSV())
}
</code></pre>
<p>Both approaches produce the same physical plan and results.</p>
<h2 id="lazy-evaluation"><a class="header" href="#lazy-evaluation">Lazy Evaluation</a></h2>
<p>Notice that building a DataFrame does not execute anything. The DataFrame just holds a logical plan. Execution happens only when you call <code>execute()</code> and consume the resulting sequence.</p>
<p>This lazy evaluation has benefits:</p>
<ul>
<li>The optimizer sees the complete query before execution</li>
<li>Errors in the plan are caught before processing starts</li>
<li>Resources are not allocated until needed</li>
</ul>
<h2 id="consuming-results"><a class="header" href="#consuming-results">Consuming Results</a></h2>
<p>The <code>execute()</code> method returns <code>Sequence&lt;RecordBatch&gt;</code>. You can process results in several ways:</p>
<p>Iterate batches:</p>
<pre><code class="language-kotlin">ctx.execute(df).forEach { batch -&gt;
    // Process each batch
}
</code></pre>
<p>Collect all results:</p>
<pre><code class="language-kotlin">val allBatches = ctx.execute(df).toList()
</code></pre>
<p>Take only what you need:</p>
<pre><code class="language-kotlin">val firstBatch = ctx.execute(df).first()
</code></pre>
<p>Because <code>Sequence</code> is lazy, taking only the first batch avoids computing subsequent batches. This matters for queries with <code>LIMIT</code>.</p>
<h2 id="example-nyc-taxi-data"><a class="header" href="#example-nyc-taxi-data">Example: NYC Taxi Data</a></h2>
<p>Let us run a real query against the NYC Taxi dataset, a common benchmark dataset with millions of rows.</p>
<pre><code class="language-kotlin">val ctx = ExecutionContext(mapOf())
ctx.registerCsv("tripdata", "/data/yellow_tripdata_2019-01.csv")

val start = System.currentTimeMillis()

val df = ctx.sql("""
    SELECT passenger_count, MAX(fare_amount)
    FROM tripdata
    GROUP BY passenger_count
""")

ctx.execute(df).forEach { batch -&gt;
    println(batch.toCSV())
}

println("Query took ${System.currentTimeMillis() - start} ms")
</code></pre>
<p>Output:</p>
<pre><code>passenger_count,MAX
1,623259.86
2,492.5
3,350.0
4,500.0
5,760.0
6,262.5
7,78.0
8,87.0
9,92.0
0,36090.3

Query took 6740 ms
</code></pre>
<h2 id="the-impact-of-optimization"><a class="header" href="#the-impact-of-optimization">The Impact of Optimization</a></h2>
<p>To see how much the optimizer helps, we can bypass it:</p>
<pre><code class="language-kotlin">// With optimization (normal path)
val optimizedPlan = Optimizer().optimize(df.logicalPlan())
val physicalPlan = QueryPlanner().createPhysicalPlan(optimizedPlan)
// Query took 6740 ms

// Without optimization
val physicalPlan = QueryPlanner().createPhysicalPlan(df.logicalPlan())
// Query took 36090 ms
</code></pre>
<p>The unoptimized query takes about five times longer. The difference comes from projection push-down: the optimized plan reads only the columns it needs (<code>passenger_count</code>, <code>fare_amount</code>), while the unoptimized plan reads all 17 columns from the CSV file.</p>
<p>For wider tables or more selective filters, the optimization impact would be even greater.</p>
<h2 id="comparison-with-apache-spark"><a class="header" href="#comparison-with-apache-spark">Comparison with Apache Spark</a></h2>
<p>For reference, here is the same query in Apache Spark:</p>
<pre><code class="language-scala">val spark = SparkSession.builder()
    .master("local[1]")  // Single thread for fair comparison
    .getOrCreate()

val tripdata = spark.read
    .option("header", "true")
    .schema(schema)
    .csv("/data/yellow_tripdata_2019-01.csv")

tripdata.createOrReplaceTempView("tripdata")

val df = spark.sql("""
    SELECT passenger_count, MAX(fare_amount)
    FROM tripdata
    GROUP BY passenger_count
""")

df.show()
// Query took 14418 ms
</code></pre>
<p>KQuery’s performance is competitive for this query. Spark has more overhead for small-to-medium datasets but scales better to very large datasets through its distributed execution capabilities.</p>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>Errors can occur at any stage:</p>
<ul>
<li>Parsing: Syntax errors in SQL</li>
<li>Planning: Unknown table or column names, type mismatches</li>
<li>Execution: Runtime errors like division by zero, file not found</li>
</ul>
<p>KQuery currently throws exceptions for errors. A production system would provide structured error types with source locations and helpful messages.</p>
<h2 id="what-we-have-built"><a class="header" href="#what-we-have-built">What We Have Built</a></h2>
<p>At this point, we have a working query engine that can:</p>
<ul>
<li>Read CSV files</li>
<li>Execute SQL queries</li>
<li>Execute DataFrame queries</li>
<li>Optimize query plans</li>
<li>Process data in batches</li>
</ul>
<p>The remaining chapters cover more advanced topics: parallel execution within a single machine, and distributed execution across a cluster.</p>
<p><em>This book is also available for purchase in ePub, MOBI, and PDF format from <a href="https://leanpub.com/how-query-engines-work">https://leanpub.com/how-query-engines-work</a></em></p>
<p><strong>Copyright © 2020-2025 Andy Grove. All rights reserved.</strong></p>

                        
</body>
</html>
["```rs\nSELECT name, salary FROM employees WHERE department = 'Engineering' \n```", "```rs\ninterface DataSource {\n\n  / Return the schema for the underlying data source */\n  fun schema(): Schema\n\n  / Scan the data source, selecting the specified columns */\n  fun scan(projection: List<String>): Sequence<RecordBatch>\n} \n```", "```rs\nid,name,department,salary\n1,Alice,Engineering,95000\n2,Bob,Sales,87000\n3,Carol,Engineering,102000 \n```", "```rs\nclass CsvDataSource(\n    val filename: String,\n    val schema: Schema?,\n    private val hasHeaders: Boolean,\n    private val batchSize: Int\n) : DataSource {\n\n  private val finalSchema: Schema by lazy { schema ?: inferSchema() }\n\n  override fun schema(): Schema {\n    return finalSchema\n  }\n  // ...\n} \n```", "```rs\noverride fun scan(projection: List<String>): Sequence<RecordBatch> {\n  val readSchema =\n      if (projection.isNotEmpty()) {\n        finalSchema.select(projection)\n      } else {\n        finalSchema\n      }\n\n  val parser = buildParser(settings)\n  parser.beginParsing(file.inputStream().reader())\n\n  return ReaderAsSequence(readSchema, parser, batchSize)\n} \n```", "```rs\nwhen (vector) {\n  is IntVector ->\n      rows.withIndex().forEach { row ->\n        val valueStr = row.value.getValue(field.value.name, \"\").trim()\n        if (valueStr.isEmpty()) {\n          vector.setNull(row.index)\n        } else {\n          vector.set(row.index, valueStr.toInt())\n        }\n      }\n  is Float8Vector ->\n      rows.withIndex().forEach { row ->\n        val valueStr = row.value.getValue(field.value.name, \"\")\n        if (valueStr.isEmpty()) {\n          vector.setNull(row.index)\n        } else {\n          vector.set(row.index, valueStr.toDouble())\n        }\n      }\n  // ... other types\n} \n```", "```rs\nclass ParquetDataSource(private val filename: String) : DataSource {\n\n  override fun schema(): Schema {\n    return ParquetScan(filename, listOf()).use {\n      val arrowSchema = SchemaConverter().fromParquet(it.schema).arrowSchema\n      SchemaConverter.fromArrow(arrowSchema)\n    }\n  }\n\n  override fun scan(projection: List<String>): Sequence<RecordBatch> {\n    return ParquetScan(filename, projection)\n  }\n} \n```", "```rs\nclass InMemoryDataSource(\n    val schema: Schema,\n    val data: List<RecordBatch>\n) : DataSource {\n\n  override fun schema(): Schema {\n    return schema\n  }\n\n  override fun scan(projection: List<String>): Sequence<RecordBatch> {\n    val projectionIndices =\n        projection.map { name -> schema.fields.indexOfFirst { it.name == name } }\n    return data.asSequence().map { batch ->\n      RecordBatch(schema, projectionIndices.map { i -> batch.field(i) })\n    }\n  }\n} \n```", "```rs\nval ctx = ExecutionContext()\nctx.registerCsv(\"employees\", \"data/employees.csv\")\nctx.registerParquet(\"sales\", \"data/sales.parquet\")\n\nval result = ctx.sql(\"SELECT * FROM employees e JOIN sales s ON e.id = s.employee_id\") \n```"]
["```rs\nclass ExecutionContext(val settings: Map<String, String>) {\n\n    private val tables = mutableMapOf<String, DataFrame>()\n\n    fun sql(sql: String): DataFrame {\n        val tokens = SqlTokenizer(sql).tokenize()\n        val ast = SqlParser(tokens).parse() as SqlSelect\n        return SqlPlanner().createDataFrame(ast, tables)\n    }\n\n    fun csv(filename: String): DataFrame {\n        return DataFrameImpl(Scan(filename, CsvDataSource(filename, ...), listOf()))\n    }\n\n    fun register(tablename: String, df: DataFrame) {\n        tables[tablename] = df\n    }\n\n    fun execute(plan: LogicalPlan): Sequence<RecordBatch> {\n        val optimizedPlan = Optimizer().optimize(plan)\n        val physicalPlan = QueryPlanner().createPhysicalPlan(optimizedPlan)\n        return physicalPlan.execute()\n    }\n} \n```", "```rs\nSQL String\n    ↓ tokenize\nTokens\n    ↓ parse\nSQL AST\n    ↓ plan\nLogical Plan\n    ↓ optimize\nOptimized Logical Plan\n    ↓ physical planning\nPhysical Plan\n    ↓ execute\nSequence<RecordBatch> \n```", "```rs\nval tokens = SqlTokenizer(sql).tokenize()\nval ast = SqlParser(tokens).parse() \n```", "```rs\nval logicalPlan = SqlPlanner().createDataFrame(ast, tables).logicalPlan() \n```", "```rs\nval optimizedPlan = Optimizer().optimize(logicalPlan) \n```", "```rs\nval physicalPlan = QueryPlanner().createPhysicalPlan(optimizedPlan) \n```", "```rs\nval results: Sequence<RecordBatch> = physicalPlan.execute() \n```", "```rs\nval ctx = ExecutionContext(mapOf())\n\n// Register a CSV file as a table\nctx.registerCsv(\"employees\", \"/data/employees.csv\")\n\n// Execute a SQL query\nval df = ctx.sql(\"\"\"\n    SELECT department, AVG(salary) as avg_salary\n    FROM employees\n    WHERE state = 'CO'\n    GROUP BY department\n\"\"\")\n\n// Execute and print results\nctx.execute(df).forEach { batch ->\n    println(batch.toCSV())\n} \n```", "```rs\nval ctx = ExecutionContext(mapOf())\n\nval results = ctx.csv(\"/data/employees.csv\")\n    .filter(col(\"state\") eq lit(\"CO\"))\n    .aggregate(\n        listOf(col(\"department\")),\n        listOf(avg(col(\"salary\")))\n    )\n\nctx.execute(results).forEach { batch ->\n    println(batch.toCSV())\n} \n```", "```rs\nctx.execute(df).forEach { batch ->\n    // Process each batch\n} \n```", "```rs\nval allBatches = ctx.execute(df).toList() \n```", "```rs\nval firstBatch = ctx.execute(df).first() \n```", "```rs\nval ctx = ExecutionContext(mapOf())\nctx.registerCsv(\"tripdata\", \"/data/yellow_tripdata_2019-01.csv\")\n\nval start = System.currentTimeMillis()\n\nval df = ctx.sql(\"\"\"\n    SELECT passenger_count, MAX(fare_amount)\n    FROM tripdata\n    GROUP BY passenger_count\n\"\"\")\n\nctx.execute(df).forEach { batch ->\n    println(batch.toCSV())\n}\n\nprintln(\"Query took ${System.currentTimeMillis() - start} ms\") \n```", "```rs\npassenger_count,MAX\n1,623259.86\n2,492.5\n3,350.0\n4,500.0\n5,760.0\n6,262.5\n7,78.0\n8,87.0\n9,92.0\n0,36090.3\n\nQuery took 6740 ms \n```", "```rs\n// With optimization (normal path)\nval optimizedPlan = Optimizer().optimize(df.logicalPlan())\nval physicalPlan = QueryPlanner().createPhysicalPlan(optimizedPlan)\n// Query took 6740 ms\n\n// Without optimization\nval physicalPlan = QueryPlanner().createPhysicalPlan(df.logicalPlan())\n// Query took 36090 ms \n```", "```rs\nval spark = SparkSession.builder()\n    .master(\"local[1]\")  // Single thread for fair comparison\n    .getOrCreate()\n\nval tripdata = spark.read\n    .option(\"header\", \"true\")\n    .schema(schema)\n    .csv(\"/data/yellow_tripdata_2019-01.csv\")\n\ntripdata.createOrReplaceTempView(\"tripdata\")\n\nval df = spark.sql(\"\"\"\n    SELECT passenger_count, MAX(fare_amount)\n    FROM tripdata\n    GROUP BY passenger_count\n\"\"\")\n\ndf.show()\n// Query took 14418 ms \n```"]